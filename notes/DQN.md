# DQN

老实说论文没怎么看懂, 就很离谱

## 算法

假设输入的特征是图片, 第$t$时间为$x_t$, 假设在$T$时终止. 那么序列$s_t$定义为

$$
s_t=x_1,a_1,x_2,a_2,...,x_{t-1},a_{t-1},x_t
$$

定义经历$e_t$为

$$
e_t=(s_t,a_t,r_t,s_{t+1})
$$

有这种东西的原因之一是, 实际中进行模拟挺麻烦的, 因此需要把模拟序列存好, 训练时可以做回放.

而RL本质是要拟合action-value函数$Q(s,a)$, 这里能看出$s$是非定长的, 随着时间步增大$s$会变长的. 原文考虑到非定长NN比较麻烦, 因此这里会用一个$\phi$约束成定长

$$
\phi(s)
$$

上述定义搞定后, 算法就比较好介绍了

- 初始化, $s_1=\{x_1\}$, 后用$\phi$处理一下成定长, 令$\phi_1=\phi(s_1)$

- 

## PAC

PAC可学习, 需要有算法, 能够在数据量足够的情况下, 对足够小的泛化误差提供足够高的成功概率.

这个在周志华的书里加了个PAC辨识. 让整体更好理解一点.

VC维这玩意的话, 比较好理解, 对我而言就是针对线性分类器不能分类亦或有了一个较为可靠的解释.

rademacher复杂度这块, 定义看懂勉勉强强.