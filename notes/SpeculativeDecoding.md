# Speculative Decoding

这给的2个paper都没什么好说, 基本思想是, 用一个小模型做infer, 然后生成几个token后用大模型检验, 大模型会从错误的地方生成一个不同的token, 小模型再从这个地方重新做infer, 生成几个token, 这样往复即可. 第一个paper就是原始的这个思想, 第二个拓展到可以使用多个小模型, 照上述方法生成的token不一定相同, 这时候可以汇集成一个树. 后面的用大模型infer时, 需要将树上token按拓扑序排列, 然后attention mask也要注意只atten根到自己的, 最后就是位置编码, 是节点深度.

这个方法问题是好像做不了batch? 因为随着纠错进行, batch内部结果会有很大区别, 要么就借鉴Orca的方案, 再加个pipeline并行. 但不管怎么说, 其跑实验时候, 第一篇paper按batch为1来做的.

总体来说这一整套方案比较偏NLP, 根本不涉及部署啥的. 但想想其和其他方案结合还是挺有前景的, 比如简单的结合一下Orca, 应该能进一步降低Latency.
